{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../datasets/heart_disease_cleaned.csv')\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test_scaled)\n",
    "y_proba_logreg = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = nb.predict(X_test_scaled)\n",
    "y_proba_nb = nb.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"=== Logistic Regression ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_logreg):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_logreg))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_logreg):.4f}\")\n",
    "\n",
    "print(\"\\n=== Naive Bayes ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_nb):.4f}\")\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_logreg), annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title(\"Logistic Regression - Confusion Matrix\")\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_nb), annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title(\"Naive Bayes - Confusion Matrix\")\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curves\n",
    "fpr_logreg, tpr_logreg, _ = roc_curve(y_test, y_proba_logreg)\n",
    "fpr_nb, tpr_nb, _ = roc_curve(y_test, y_proba_nb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_logreg, tpr_logreg, label=f\"Logistic Regression (AUC = {roc_auc_score(y_test, y_proba_logreg):.2f})\")\n",
    "plt.plot(fpr_nb, tpr_nb, label=f\"Naive Bayes (AUC = {roc_auc_score(y_test, y_proba_nb):.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Tabular Comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Naive Bayes'],\n",
    "    'Accuracy': [accuracy_score(y_test, y_pred_logreg), accuracy_score(y_test, y_pred_nb)],\n",
    "    'ROC-AUC': [roc_auc_score(y_test, y_proba_logreg), roc_auc_score(y_test, y_proba_nb)]\n",
    "})\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
